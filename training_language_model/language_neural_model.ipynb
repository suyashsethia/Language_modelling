{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ruk jaa bhia \n"
     ]
    }
   ],
   "source": [
    "from Tokenizer import *\n",
    "\n",
    "path_to_curpus = 'sample.txt'\n",
    "\n",
    "#convert the corpus to a list of tokens\n",
    "sentences ,freq ,tokens = tokenize(path_to_curpus )\n",
    "print(\"ruk jaa bhia \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "<torch.utils.data.dataset.Subset object at 0x7f8cf7fc94b0> <torch.utils.data.dataset.Subset object at 0x7f8cf7fc8970> <torch.utils.data.dataset.Subset object at 0x7f8cf81451b0>\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# split into train , test and validation sets with random shuffling\n",
    "train_size = 0.8\n",
    "test_size = 0.1\n",
    "validation_size = 0.1\n",
    "print(len(sentences))\n",
    "#remove 6 senteces from the end of the list\n",
    "# sentences = sentences[:-6]\n",
    "# prit\n",
    "proportions = [.75, .10, .15]\n",
    "lengths = [int(p * len(sentences)) for p in proportions]\n",
    "lengths[-1] = len(sentences) - sum(lengths[:-1])\n",
    "train , test , validation = torch.utils.data.random_split(sentences, lengths)\n",
    "# train , test , validation = torch.utils.data.random_split(sentences , [int(train_size*len(sentences)) , int(test_size*len(sentences)) , int(validation_size*len(sentences))])\n",
    "\n",
    "print(train , test , validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max sequence length: 11\n",
      "[[1, 1, 1, 2, 3, 4, 5, 6, 7], [1, 1, 1, 8, 9, 4, 5, 6, 10, 11, 7], [1, 1, 1, 12, 9, 13, 7], [1, 1, 1, 7], [1, 1, 1, 2, 3, 4, 5, 14, 7], [1, 1, 1, 12, 9, 4, 5, 6, 10, 14, 7]]\n"
     ]
    }
   ],
   "source": [
    "#give unique index to each token and create a vocabulary\n",
    "# vocab = {token:idx for idx,token in enumerate(tokens)}\n",
    "vocab = {\"<pad>\": 0}  # Add a <pad> token to the vocab\n",
    "inputs = []\n",
    "max_length = 0  # Track the maximum sequence length\n",
    "\n",
    "\n",
    "for sentence in train :\n",
    "    sentence = \"<s> \"*3 +sentence + \" </s>\"\n",
    "    sentence_indexes = []\n",
    "    words = sentence.split()\n",
    "    for word in words:\n",
    "        if word not in vocab:\n",
    "            vocab[word] = len(vocab)\n",
    "        sentence_indexes.append(vocab[word])\n",
    "    inputs.append(sentence_indexes)\n",
    "    max_length = max(max_length, len(sentence_indexes))\n",
    "    \n",
    "print(\"Max sequence length:\", max_length)\n",
    "# print(\"Shape of inputs before padding:\", torch.LongTensor(inputs).shape)\n",
    "\n",
    "# print(vocab)\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 1, 1, 2, 3, 4, 5, 6, 7, 0, 0], [1, 1, 1, 8, 9, 4, 5, 6, 10, 11, 7], [1, 1, 1, 12, 9, 13, 7, 0, 0, 0, 0], [1, 1, 1, 7, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 2, 3, 4, 5, 14, 7, 0, 0], [1, 1, 1, 12, 9, 4, 5, 6, 10, 14, 7]]\n"
     ]
    }
   ],
   "source": [
    "# Pad the input sequences with the <pad> token to make them all the same length\n",
    "for i in range(len(inputs)):\n",
    "    inputs[i] = inputs[i] + [vocab[\"<pad>\"]] * (max_length - len(inputs[i]))\n",
    "\n",
    "\n",
    "print(inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1] 2\n",
      "[1, 1, 2] 3\n",
      "[1, 2, 3] 4\n",
      "[2, 3, 4] 5\n",
      "[3, 4, 5] 6\n",
      "[4, 5, 6] 7\n",
      "[1, 1, 1] 8\n",
      "[1, 1, 8] 9\n",
      "[1, 8, 9] 4\n",
      "[8, 9, 4] 5\n",
      "[9, 4, 5] 6\n",
      "[4, 5, 6] 10\n",
      "[5, 6, 10] 11\n",
      "[6, 10, 11] 7\n",
      "[1, 1, 1] 12\n",
      "[1, 1, 12] 9\n",
      "[1, 12, 9] 13\n",
      "[12, 9, 13] 7\n",
      "[1, 1, 1] 7\n",
      "[1, 1, 1] 2\n",
      "[1, 1, 2] 3\n",
      "[1, 2, 3] 4\n",
      "[2, 3, 4] 5\n",
      "[3, 4, 5] 14\n",
      "[4, 5, 14] 7\n",
      "[1, 1, 1] 12\n",
      "[1, 1, 12] 9\n",
      "[1, 12, 9] 4\n",
      "[12, 9, 4] 5\n",
      "[9, 4, 5] 6\n",
      "[4, 5, 6] 10\n",
      "[5, 6, 10] 14\n",
      "[6, 10, 14] 7\n",
      "tensor([[ 1,  1,  1,  2,  3,  4,  5,  6,  7,  0],\n",
      "        [ 1,  1,  1,  8,  9,  4,  5,  6, 10, 11],\n",
      "        [ 1,  1,  1, 12,  9, 13,  7,  0,  0,  0],\n",
      "        [ 1,  1,  1,  7,  0,  0,  0,  0,  0,  0],\n",
      "        [ 1,  1,  1,  2,  3,  4,  5, 14,  7,  0],\n",
      "        [ 1,  1,  1, 12,  9,  4,  5,  6, 10, 14]]) tensor([[ 1,  1,  2,  3,  4,  5,  6,  7,  0,  0],\n",
      "        [ 1,  1,  8,  9,  4,  5,  6, 10, 11,  7],\n",
      "        [ 1,  1, 12,  9, 13,  7,  0,  0,  0,  0],\n",
      "        [ 1,  1,  7,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 1,  1,  2,  3,  4,  5, 14,  7,  0,  0],\n",
      "        [ 1,  1, 12,  9,  4,  5,  6, 10, 14,  7]])\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "#convert to LongTensor\n",
    "prefixes = []\n",
    "labels = []\n",
    "for i in range(len(inputs)):\n",
    "    for j in range(len(inputs[i])-3):\n",
    "        if(inputs[i][j+3]==0): \n",
    "           break          \n",
    "        prefix = inputs[i][j:j+3]\n",
    "        label = inputs[i][j+3]\n",
    "        # print(prefix , label)\n",
    "        prefixes.append(prefix)\n",
    "        labels.append(label)\n",
    "    #3-gram prefix\n",
    "\n",
    "\n",
    "    \n",
    "prefixes  = torch.LongTensor([sentence[:-1] for sentence in inputs])\n",
    "labels = torch.LongTensor([sentence[1:] for sentence in inputs])\n",
    "\n",
    "\n",
    "print(prefixes , labels)\n",
    "\n",
    "#Define inputs and outputs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (6x50 and 10x12)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 51\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[39m# training loop\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_epochs):\n\u001b[0;32m---> 51\u001b[0m     logits \u001b[39m=\u001b[39m network(prefixes)\n\u001b[1;32m     52\u001b[0m     loss \u001b[39m=\u001b[39m loss_fn(logits, labels)\n\u001b[1;32m     53\u001b[0m     \u001b[39m# now let's update our params to make the loss smaller\u001b[39;00m\n\u001b[1;32m     54\u001b[0m     \u001b[39m# step 1: compute gradient (partial derivs of loss WRT parameters)\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[33], line 30\u001b[0m, in \u001b[0;36mNLM.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     26\u001b[0m concat_embs \u001b[39m=\u001b[39m embs\u001b[39m.\u001b[39mview(batch_size, window_size \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39md_emb)\n\u001b[1;32m     27\u001b[0m \u001b[39m# print('concatenated embs size:', concat_embs.size())\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \n\u001b[1;32m     29\u001b[0m \u001b[39m# now, we project this to the hidden space\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m hiddens \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mW_hid(concat_embs)\n\u001b[1;32m     31\u001b[0m \u001b[39m# print('hidden size:', hiddens.size())\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \n\u001b[1;32m     33\u001b[0m \u001b[39m# finally, project hiddens to vocabulary space\u001b[39;00m\n\u001b[1;32m     34\u001b[0m outs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mW_out(hiddens)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (6x50 and 10x12)"
     ]
    }
   ],
   "source": [
    "# onto defining the network!\n",
    "import torch.nn as nn\n",
    "\n",
    "class NLM(nn.Module):\n",
    "    # two things that you need to do\n",
    "    # 1. init function (initializes all the **parameters** of the network)\n",
    "    # 2. forward function (defines the forward propagation computations)\n",
    "\n",
    "    def __init__(self, d_embedding, d_hidden, window_size, len_vocab):\n",
    "        super(NLM, self).__init__() # init the base Module class\n",
    "        self.d_emb = d_embedding\n",
    "        self.embeddings = nn.Embedding(len_vocab, d_embedding)\n",
    "\n",
    "        # concatenated embeddings > hidden\n",
    "        self.W_hid = nn.Linear(d_embedding*window_size,  d_hidden)\n",
    "\n",
    "        # hidden > output probability distribution over vocab\n",
    "        self.W_out = nn.Linear(d_hidden, len_vocab)\n",
    "\n",
    "    def forward(self, input): # each input will be a batch of prefixes (in this case 4)\n",
    "        batch_size, window_size = input.size()\n",
    "        embs = self.embeddings(input) # 4 x 2 x 5\n",
    "        # print('embedding size:', embs.size())\n",
    "\n",
    "        # next,  we want to concatenate the prefix embeddings together\n",
    "        concat_embs = embs.view(batch_size, window_size * self.d_emb)\n",
    "        # print('concatenated embs size:', concat_embs.size())\n",
    "\n",
    "        # now, we project this to the hidden space\n",
    "        hiddens = self.W_hid(concat_embs)\n",
    "        # print('hidden size:', hiddens.size())\n",
    "\n",
    "        # finally, project hiddens to vocabulary space\n",
    "        outs = self.W_out(hiddens)\n",
    "        # print('output size:', outs.size())\n",
    "\n",
    "        return outs # return unnormalized probability, also known as **logits**\n",
    "\n",
    "\n",
    "network = NLM(d_embedding=5, d_hidden=12, window_size=2, len_vocab=len(vocab))\n",
    "\n",
    "num_epochs = 100 # how many times we are going to go through our entire training dataset\n",
    "learning_rate = 0.1 # we can modify this if training isn't converging, etc.\n",
    "loss_fn = nn.CrossEntropyLoss() # average cross entropy loss over each prefix in batch\n",
    "\n",
    "# we will use vanilla gradient descent, you can experiment with others like Adam\n",
    "optimizer = torch.optim.SGD(params=network.parameters(), lr=learning_rate)\n",
    "\n",
    "# training loop\n",
    "for i in range(num_epochs):\n",
    "    logits = network(prefixes)\n",
    "    loss = loss_fn(logits, labels)\n",
    "    # now let's update our params to make the loss smaller\n",
    "    # step 1: compute gradient (partial derivs of loss WRT parameters)\n",
    "    loss.backward()\n",
    "\n",
    "    # step 2: update params using gradient descent\n",
    "    optimizer.step()\n",
    "\n",
    "    # step 3: zero out the gradients for the next epoch\n",
    "    optimizer.zero_grad()\n",
    "    print('epoch: %d, loss: %0.2f' % (i, loss))\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
