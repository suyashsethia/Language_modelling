{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lstm based language model for text generation using pytorch \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "\n",
    "#load data\n",
    "df = pd.read_csv('spam.csv', encoding='latin-1')\n",
    "df = df.drop(['Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'], axis=1)\n",
    "df = df.rename(columns={'v1':'label', 'v2':'text'})\n",
    "df['label'] = df['label'].map({'ham':0, 'spam':1})\n",
    "df.head()\n",
    "\n",
    "#data preprocessing\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', ' ', text)\n",
    "    text = text.strip()\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text\n",
    "\n",
    "df['text'] = df['text'].apply(lambda x: clean_text(x))\n",
    "df.head()\n",
    "\n",
    "#split data\n",
    "X = df['text']\n",
    "y = df['label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#vectorize data\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train = vectorizer.fit_transform(X_train)\n",
    "X_test = vectorizer.transform(X_test)\n",
    "\n",
    "#convert to torch tensors\n",
    "X_train = torch.from_numpy(X_train.toarray()).float()\n",
    "X_test = torch.from_numpy(X_test.toarray()).float()\n",
    "y_train = torch.from_numpy(y_train.to_numpy()).float()\n",
    "y_test = torch.from_numpy(y_test.to_numpy()).float()\n",
    "\n",
    "#build model\n",
    "class LSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        \n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "#hyperparameters\n",
    "input_size = 6293\n",
    "hidden_size = 128\n",
    "num_layers = 2\n",
    "num_classes = 1\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10\n",
    "batch_size = 64\n",
    "\n",
    "#device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "#load data\n",
    "train = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "test = torch.utils.data.TensorDataset(X_test, y_test)\n",
    "train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "#initialize model\n",
    "model = LSTM(input_size, hidden_size, num_layers, num_classes).to(device)\n",
    "\n",
    "#loss and optimizer\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "#train model\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    for i, (data, targets) in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        targets = targets.to(device)\n",
    "        targets = targets.view(-1, 1)\n",
    "\n",
    "        #forward\n",
    "        scores = model(data)\n",
    "        loss = criterion(scores, targets)\n",
    "\n",
    "        #backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        #gradient descent or adam step\n",
    "        optimizer.step()\n",
    "\n",
    "#calculate perplexity\n",
    "def perplexity(model, data_loader):\n",
    "\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_count = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, targets in data_loader:\n",
    "            data = data.to(device)\n",
    "            targets = targets.to(device)\n",
    "            targets = targets.view(-1, 1)\n",
    "\n",
    "            scores = model(data)\n",
    "            loss = criterion(scores, targets)\n",
    "            total_loss += loss.item() * data.size(0)\n",
    "            total_count += data.size(0)\n",
    "\n",
    "    loss = total_loss / total_count\n",
    "    ppl = np.exp(loss)\n",
    "\n",
    "    return ppl\n",
    "\n",
    "#calculate accuracy\n",
    "def accuracy(model, data_loader):\n",
    "\n",
    "    model.eval()\n",
    "    total_correct = 0\n",
    "    total_count = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, targets in data_loader:\n",
    "            data = data.to(device)\n",
    "            targets = targets.to(device)\n",
    "            targets = targets.view(-1, 1)\n",
    "\n",
    "            scores = model(data)\n",
    "            predictions = scores > 0\n",
    "            total_correct += (predictions == targets).sum()\n",
    "            total_count += targets.size(0)\n",
    "\n",
    "    acc = total_correct / total_count\n",
    "\n",
    "    return acc\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
